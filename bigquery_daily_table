#!/usr/bin/env python3.4
# -*- coding: utf-8 -*-

__author__ = "Michele Berardi"
__copyright__ = "Copyright 2018, "
__license__ = "GPL"
__version__ = "1.0.1"
__maintainer__ = "Michele Berardi"
__email__ = "michele.berardi@bucksense.com"
__status__ = "Production"

import json
import logging
import os
import time
import requests
from datetime import datetime

import ndjson
import pymysql
from dateutil.relativedelta import relativedelta
from googleadwords.googleapi.bigquery.google.cloud import bigquery

#os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/var/www/html/source/bucksense_bigquery_connector/init/bucksense-rtb-199a0153cd93.json"
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "init/bucksense-rtb-199a0153cd93.json"

# TIME NOW
datenow = datetime.now().strftime('%Y-%m-%d')
date2 = datetime.today() + relativedelta(days=-2)
date3 = datetime.today() + relativedelta(days=-2)
date_start = date2.strftime('%Y-%m-%d')
date_end = date3.strftime('%Y-%m-%d')
date_query = date2.strftime('%Y%m%d')
print("START DATE ", date_start)
print("END DATE ", date_end)

# START TIME
start = time.time()

# SECTION LOG
try:
    logging.basicConfig(level=logging.DEBUG,
                        format='%(asctime)s %(levelname)-8s %(message)s',
                        datefmt='%a, %d %b %Y %H:%M:%S',
                        filename='/var/www/html/source/bucksense_bigquery_connector/logs/app.log', filemode='a')
except:
    logging.basicConfig(level=logging.DEBUG,
                        format='%(asctime)s %(levelname)-8s %(message)s',
                        datefmt='%a, %d %b %Y %H:%M:%S',
                        filename='logs/app.log', filemode='a')
logging.debug('>>>>> START TRANSACTION <<<<<')

mydb = pymysql.connect(host='10.200.40.28', user='tats', passwd='tats', db='bucksense_fb_ads', use_unicode=True,
                       charset="utf8")
cursor = mydb.cursor()

logging.debug('>>>>> check the database')


# FUNCTION QUERY MYSQL
def query():
    cursor.execute(
        "select insert_date,date,company_id,company_name FROM bucksense_bigquery_connector.campaign_structure_1 where date between '" + date_start + "' and '" + date_start + "'")
    row_headers = [x[0] for x in cursor.description]  # this will extract row headers
    data = cursor.fetchall()
    json_data = []
    for result in data:
        json_data.append(dict(zip(row_headers, result)))
    return json.dumps(json_data, indent=4, sort_keys=False, default=str, *'')  # MAGICCCCC FIXED


if __name__ == '__main__':
    # QUERY MYSQL
    result1 = query()
    # PARSE JSON
    data = json.loads(result1)
    str = json.dumps(data)
    parsed = json.loads(str)
    with open('data.json', 'w') as f:
        ndjson.dump(parsed, f)
    # IMPORT INTO BIG QUERY
    filename = 'data.json'
    client = bigquery.Client(project="bucksense-rtb")
    dataset_id = 'directopub_demo'
    table_id = 'test_daily_'+date_query
    dataset_ref = client.dataset(dataset_id)
    table_ref = dataset_ref.table(table_id)
    job_config = bigquery.LoadJobConfig()
    job_config.schema = [
        bigquery.SchemaField('insert_date', 'TIMESTAMP'),
        bigquery.SchemaField('date', 'STRING'),
        bigquery.SchemaField('company_id', 'STRING'),
        bigquery.SchemaField('company_name', 'STRING'),

    ]

    job_config._properties['load']['timePartitioning'] = {'type': 'DAY', 'field': 'insert_date',
                                                          'require_partition_filter': True}

    #table_ref = dataset_ref.table(table_id)
    #job_config = bigquery.LoadJobConfig()
    #job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE.partition(date_query)
    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON
    with open(filename, "rb") as source_file:
        job = client.load_table_from_file(source_file, dataset_ref.table(table_id), job_config=job_config)
        print(job.job_id)

        job.result()  # Waits for table load to complete.
        destination_table = client.get_table(dataset_ref.table(table_id))

    #print("Loaded {} rows into {}:{}.".format(job.output_rows, dataset_id, table_id + "$" + date_start))

    # REPORT IMPORT ON SLACK CHANNEL #DEVOPS
    print(job.output_rows)
    if job.output_rows == 0:
        report = (
            "*BIG QUERY 1* Loaded {} rows into {}:{}.".format(job.output_rows, dataset_id, table_id + "-" + date_start))
        webhook_url = 'https://hooks.slack.com/services/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'
        slack_report = {"attachments": [
            {
                "fallback": "*BIG QUERY 1*",
                "color": "#FF0000", "text": report}]}
        response = requests.post(webhook_url, json=slack_report, headers={'Content-Type': 'application/json'})
    else:
        report = (
            "*BIG QUERY 1* Loaded {} rows into {}:{}.".format(job.output_rows, dataset_id, table_id + "-" + date_start))
        webhook_url = 'https://hooks.slack.com/services/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'
        slack_report = {"attachments": [
            {
                "fallback": "*BIG QUERY 1*",
                "color": "#36a64f", "text": report}]}
        response = requests.post(webhook_url, json=slack_report, headers={'Content-Type': 'application/json'})
