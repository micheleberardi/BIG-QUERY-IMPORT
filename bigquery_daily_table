#!/usr/bin/env python3.4
# -*- coding: utf-8 -*-

__author__ = "Michele Berardi"
__copyright__ = "Copyright 2020, "
__license__ = "GPL"
__version__ = "1.0.1"
__maintainer__ = "Michele Berardi"
__email__ = "michele@linux.com"
__status__ = "Production"

import json
import logging
import time
import requests
from datetime import datetime, timedelta
import ndjson
import pymysql
from dateutil.relativedelta import relativedelta
from google.cloud import bigquery
import os
import sys
import socket
# CHECK PID
def get_lock(process_name):
    get_lock._lock_socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    try:
        get_lock._lock_socket.bind('\0' + process_name)
        print ('WRITE THE PID')
        logging.info("WRITE THE PID")
    except socket.error:
        print ('PID ALREADY EXIST')
        logging.error("PID ALREADY EXIST")
        sys.exit(0)



# FUNCTION QUERY MYSQL
def query():
    cursor.execute(
        "select insert_date,date,company_id,company_name FROM database_name.yable_name where date between '" + date_start + "' and '" + date_start + "'")
    row_headers = [x[0] for x in cursor.description]  # this will extract row headers
    data = cursor.fetchall()
    json_data = []
    for result in data:
        json_data.append(dict(zip(row_headers, result)))
    return json.dumps(json_data, indent=4, sort_keys=False, default=str, *'')  # MAGICCCCC FIXED


if __name__ == '__main__':
    #######################################################################################################
    # CONFIG SECTION
    ENV = (os.path.dirname(os.path.realpath(__file__)))

    # SECTION LOGS
    datelog = datetime.now().strftime('%Y-%m-%d')
    logging.basicConfig(level=logging.DEBUG,
                        format='%(asctime)s %(levelname)-8s %(message)s',
                        datefmt='%a, %d %b %Y %H:%M:%S',
                        filename=ENV + "/logs/ads-ingester-one" + datelog + ".log", filemode='a')

    # DATABASE
    mydb = pymysql.connect(host='10.xxx.xx.xx', user='username', passwd='password', db='database_name', use_unicode=True,charset="utf8")
    cursor = mydb.cursor()
    
    # GOOGLE TOKEN
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ENV + "/init/michelone-xxxxxxxx.json"

    #######################################################################################################
    # CHECK PROCESS PID
    logging.info("****START****")
    check_pid = get_lock('core.py')
#LOOP DATA IF YOU WANT TO IMPORT MORE DAYS
    date3 = datetime.today() + relativedelta(days=-5)
    date4 = datetime.today() + relativedelta(days=1)
    startdate_loop = date3.strftime('%Y-%m-%d')
    enddate_loop = date4.strftime('%Y-%m-%d')
    start = datetime.strptime(startdate_loop, "%Y-%m-%d")
    end = datetime.strptime(enddate_loop, "%Y-%m-%d")
    date_generated = [start + timedelta(days=x) for x in range(0, (end - start).days)]
    print(date_generated)
    for date in date_generated:
        print(date)
        date_start = date.strftime('%Y-%m-%d 00:00:00')
        date_end = date.strftime('%Y-%m-%d 23:59:59')
        data_bigquery = (date.strftime('%Y-%m-%d'))
        date_query = date.strftime('%Y%m%d')
# QUERY MYSQL
        result1 = query()
# PARSE JSON
        data = json.loads(result1)
        str = json.dumps(data)
        parsed = json.loads(str)
        with open('data.json', 'w') as f:
            ndjson.dump(parsed, f)
# IMPORT INTO BIG QUERY
        filename = 'data.json'
        client = bigquery.Client(project="client-rtb")
        dataset_id = 'dataset_demo'
        table_id = 'test_daily_'+date_query
        dataset_ref = client.dataset(dataset_id)
        table_ref = dataset_ref.table(table_id)
        job_config = bigquery.LoadJobConfig()
        job_config.schema = [
            bigquery.SchemaField('insert_date', 'TIMESTAMP'),
            bigquery.SchemaField('date', 'STRING'),
            bigquery.SchemaField('company_id', 'STRING'),
            bigquery.SchemaField('company_name', 'STRING'),

        ]

        job_config._properties['load']['timePartitioning'] = {'type': 'DAY', 'field': 'insert_date',
                                                          'require_partition_filter': True}

        job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON
        with open(filename, "rb") as source_file:
            job = client.load_table_from_file(source_file, dataset_ref.table(table_id), job_config=job_config)
            print(job.job_id)

            job.result()  # Waits for table load to complete.
            destination_table = client.get_table(dataset_ref.table(table_id))
# REPORT IMPORT ON SLACK CHANNEL #DEVOPS
        print(job.output_rows)
        if job.output_rows == 0:
            report = ("*BIG QUERY 1* Loaded {} rows into {}:{}.".format(job.output_rows, dataset_id, table_id + "-" + date_start))
            webhook_url = 'https://hooks.slack.com/services/token_slack'
            slack_report = {"attachments": [{"fallback": "*BIG QUERY 1*","color": "#FF0000", "text": report}]}
            response = requests.post(webhook_url, json=slack_report, headers={'Content-Type': 'application/json'})
        else:
            report = ("*BIG QUERY 1* Loaded {} rows into {}:{}.".format(job.output_rows, dataset_id, table_id + "-" + date_start))
            webhook_url = 'https://hooks.slack.com/services/token_slack'
            slack_report = {"attachments": [{"fallback": "*BIG QUERY 1*","color": "#36a64f", "text": report}]}
            response = requests.post(webhook_url, json=slack_report, headers={'Content-Type': 'application/json'})
